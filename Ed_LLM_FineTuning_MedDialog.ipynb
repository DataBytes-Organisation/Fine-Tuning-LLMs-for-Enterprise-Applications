{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataBytes-Organisation/Fine-Tuning-LLMs-for-Enterprise-Applications/blob/ed_branch/Ed_LLM_FineTuning_MedDialog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUb2oSz45XV4"
      },
      "source": [
        "# Personalized Healthcare QA System (Chatbot) - V3 - MedDialog IC\n",
        "#### Ed:215279167\n",
        "---\n",
        "### Objective:\n",
        "Develop a healthcare chatbot that answers patient inquiries on medications, symptoms, and  treatments while reducing hallucinations.\n",
        "\n",
        "### Update and changes:\n",
        "This updated notebook reflects changes learned from weeks 1 -7 experiments with fine-tuning. Main changes include:\n",
        "* Revert to using MedDialog dataset to validate model is being fine tuned correctly\n",
        "\n",
        "### Datasets:\n",
        "* Med Dialog IC - https://huggingface.co/datasets/lighteval/med_dialog\n",
        "\n",
        "### Task Breakdown:\n",
        "1. Train models on medical QA datasets.\n",
        "2. Fine-tune for patient-friendly responses (simplified, clear language).\n",
        "3. Ensure context-aware, regulatory-compliant answers:  \n",
        "o Implement FDA/TGA guideline alignment.\n",
        "o Develop a retrieval-based validation for generated answers.\n",
        "4. Deploy as a chatbot interface (React-based UI + API integration).\n",
        "5. Implement real-time fact-checking:  \n",
        "o Confidence score visualization (green = high confidence, yellow = medium, red = low\n",
        "confidence).\n",
        "o Integration with PubMed and trusted medical sources.\n",
        "\n",
        "### Models to Use:\n",
        "â€¢ Llama-2 7B\n",
        "\n",
        "### Evaluation Metrics:\n",
        "* Rouge, BLEU, Meteor, BertScore, Manual inspection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C8_1rPk-OkG"
      },
      "source": [
        "---\n",
        "## 1. Import libraries and model to prepare for fine tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0NvJyuYeGBJ"
      },
      "outputs": [],
      "source": [
        "!pip install datasets requests bitsandbytes accelerate peft trl sentencepiece wandb transformers evaluate rouge_score bert-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIeIbt2M9vut"
      },
      "source": [
        "### IMPORTANT: Restart Colab runtime after PIP install!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Implement base model and imports\n",
        "First step is to import all necessary libraries and implement base model, to validate it can generate a response from prompt."
      ],
      "metadata": {
        "id": "lxw-QITQ9iW0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqaBKB9e4Vhe"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, TaskType\n",
        "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from bert_score import BERTScorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4wnNrTyAJKX"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "project_name = \"ed_medical\"\n",
        "hf_username = \"digitalblue\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8r6Ypk6_x0S"
      },
      "outputs": [],
      "source": [
        "# log into hugging face and wandb\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)\n",
        "\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = \"ed_medical\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if True else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcmAaL_04ONh"
      },
      "outputs": [],
      "source": [
        "# quantisation config to use less memory when loading model\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vCtSkep6wCb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj4U50PY7o0Z"
      },
      "outputs": [],
      "source": [
        "memory = model.get_memory_footprint() / 1e6\n",
        "print(f\"Memory footprint: {memory:,.1f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcZz_RQU-N0l"
      },
      "outputs": [],
      "source": [
        "# model architecture\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJK_xYZlCUbj"
      },
      "outputs": [],
      "source": [
        "# initialise tokenizer and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALuROA3g6cPq"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful personalised medical assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"How can I get rid of the flu?\"}\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U73PHhEU-ZaH"
      },
      "outputs": [],
      "source": [
        "# verify base model generates response\n",
        "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(inputs, max_new_tokens=250)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1KeJ2moW0s"
      },
      "source": [
        "## 3. Dataset analysis and preprocessing for Llama\n",
        "The datasets need to be analysed and prepared for fine-tuning the model. This includes removing invalid data and formatting into the required format with special tokens the Llama 2 model requires. Dataset acquired from HuggingFace\n",
        "* MedDialog_IC - https://huggingface.co/datasets/lighteval/med_dialog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U2t0GudQnjq"
      },
      "outputs": [],
      "source": [
        "# format input from datasets into prompt format required by llama model\n",
        "def format_llama_prompt(user_message, model_answer):\n",
        "  prompt = '<s>[INST] ' # special token - commence instruct\n",
        "  prompt += user_message.strip()\n",
        "  prompt += ' [/INST] ' # special token - end instruct\n",
        "  prompt += model_answer.strip()\n",
        "  prompt += ' </s>'# special token - end\n",
        "  return prompt\n",
        "\n",
        "def format_llama_prompt_with_context(user_message, model_answer, context):\n",
        "  prompt = '<s>[INST] Given this context: ' # special token - commence instruct\n",
        "  prompt += context['contexts'][0].strip()\n",
        "  prompt += ' Question: '\n",
        "  prompt += user_message.strip()\n",
        "  prompt += ' [/INST] ' # special token - end instruct\n",
        "  prompt += model_answer.strip()\n",
        "  prompt += ' </s>'# special token - end\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load med_dialog dataset\n",
        "med_dialog_ic = load_dataset(\"lighteval/med_dialog\", \"icliniq\")"
      ],
      "metadata": {
        "id": "YgUFrzDILafn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_dialog_ic"
      ],
      "metadata": {
        "id": "Bv7ZV8iYNqFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_dialog_ic['train'][100]"
      ],
      "metadata": {
        "id": "zfOeeGuELf1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# med_dialog data requires prep to parse patient/doctor chat in src\n",
        "def prep_med_dialog(src_dialog):\n",
        "  init_split = src_dialog.split('Patient: ')\n",
        "  init_split = [item for item in init_split if item] # remove empty item introduced by init split\n",
        "  if len(init_split) > 2:\n",
        "    print('Longer dialog: ' + str(init_split))\n",
        "    return None\n",
        "  else:\n",
        "    #print(next_split)\n",
        "    next_split = init_split[0].split(' Doctor:')\n",
        "    if len(next_split) > 1:\n",
        "      return format_llama_prompt(next_split[0], next_split[1])\n",
        "    else:\n",
        "      print(\"Invalid dialog: \" + str(src_dialog))\n",
        "      return None\n"
      ],
      "metadata": {
        "id": "7QiNVmVTMK20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create subset of med_dialag src value\n",
        "med_dialog_ic_sub = med_dialog_ic['train']['src']"
      ],
      "metadata": {
        "id": "uy6FWlRKMM-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create list of formatted prompts\n",
        "med_dialog_ic_as_prompt = []\n",
        "for index, dialog in enumerate(med_dialog_ic_sub):\n",
        "  med_dialog_ic_as_prompt.append(prep_med_dialog(dialog))"
      ],
      "metadata": {
        "id": "v9q_vR0yMThU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "med_dialog_ic_as_prompt[100]"
      ],
      "metadata": {
        "id": "MtaqpeTVMZ93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coPjx5TLGwyn"
      },
      "outputs": [],
      "source": [
        "print(len(med_dialog_ic_as_prompt))\n",
        "print(med_dialog_ic_as_prompt[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts8u9Dp2tlqu"
      },
      "outputs": [],
      "source": [
        "# collect all formatted data into one dataset\n",
        "prompt_dataset = med_dialog_ic_as_prompt\n",
        "print(len(prompt_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjpG35k6TiiV"
      },
      "outputs": [],
      "source": [
        "# convert to huggingface dataset, create splits and push to hub\n",
        "prompt_hf_dataset = Dataset.from_dict({\"text\": prompt_dataset})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybrNf0dVVgXR"
      },
      "outputs": [],
      "source": [
        "ds_train = prompt_hf_dataset.train_test_split(test_size=0.2, seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVyQtGUwZqLb"
      },
      "outputs": [],
      "source": [
        "ds_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca4Mju9PVtr3"
      },
      "outputs": [],
      "source": [
        "ds_test = ds_train['test'].train_test_split(test_size=0.5, seed=42)\n",
        "ds_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np2CXY-iPUil"
      },
      "outputs": [],
      "source": [
        "ds_splits = DatasetDict({\n",
        "    'train': ds_train['train'],\n",
        "    'validation': ds_test['train'],\n",
        "    'test': ds_test['test']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4bWyjWeU_ut"
      },
      "outputs": [],
      "source": [
        "ds_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V9R707vR9HH"
      },
      "outputs": [],
      "source": [
        "# push to hugging face\n",
        "ds_splits.push_to_hub(f\"{hf_username}/{project_name}-med-dialog-ic\", private=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwQ0tJ6SpEhu"
      },
      "source": [
        "## 4. Model Training Pipeline\n",
        "The model training pipeline is setup for fine-tuning. During this process additional data processing was required to filter out data with execessive token length that would exhaust GPU resources.\n",
        "\n",
        "Initially four variants(A,B,C,D) of the model were fine-tuned on combination dataset, however focus is now on a single model trained on Med Dialog IC dataset. It maintains integration with HuggingFace and Weight and Bias platform so models and runs are saved for later retrieval.\n",
        "\n",
        "The models were fine-tuned on the **train** set, and evaluated with **validation** set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIkMaVEF2fsq"
      },
      "outputs": [],
      "source": [
        "qa_dataset = load_dataset(\"digitalblue/ed_medical-med-dialog-ic\")\n",
        "qa_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw4YL5RSz8Ue"
      },
      "outputs": [],
      "source": [
        "# getting token lengths from training dataset\n",
        "train_length = []\n",
        "train_token_length = []\n",
        "for item in qa_dataset['train']:\n",
        "  if item['text']:\n",
        "    train_length.append(len(item['text']))\n",
        "    tokens = tokenizer.encode(item['text'])\n",
        "    train_token_length.append(len(tokens))\n",
        "  else:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNjYg5Dj1k-J"
      },
      "outputs": [],
      "source": [
        "len(train_token_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsGzUkKO147u"
      },
      "outputs": [],
      "source": [
        "# print the min, max and average character lengths and token count lengths for training data\n",
        "print(min(train_length), max(train_length), sum(train_length)/len(train_length))\n",
        "print(min(train_token_length), max(train_token_length), sum(train_token_length)/len(train_token_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSy_f8UQcKOO"
      },
      "outputs": [],
      "source": [
        "# plot the token lengths\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.hist(train_token_length, rwidth=0.7, bins=100)\n",
        "plt.xlabel('Token Length')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Token Length Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh9GxruEX-Zn"
      },
      "outputs": [],
      "source": [
        "# avg is now approx 311, but will keep 350 tokens, therefore will create new datasets\n",
        "# to filter out any rows above this token count\n",
        "MAX_TOKENS = 350\n",
        "DATASET_SIZE = 1000\n",
        "def get_filtered_dataset(dataset, max_tokens, size):\n",
        "  count = 0\n",
        "  train_size = round(size * 0.9) # 90% for train\n",
        "  valid_size = round(size * 0.1) # 10% for eval\n",
        "  print(f\"Train size: {train_size}, Eval size: {valid_size}\")\n",
        "\n",
        "  filtered_train_dataset = []\n",
        "  for item in dataset['train']:\n",
        "    if item['text']:\n",
        "      tokens = tokenizer.encode(item['text'])\n",
        "      if len(tokens) < max_tokens:\n",
        "        filtered_train_dataset.append(item)\n",
        "        count += 1\n",
        "      if count >= train_size:\n",
        "        break\n",
        "\n",
        "  count = 0\n",
        "  filtered_eval_dataset = []\n",
        "  for item in dataset['validation']:\n",
        "    if item['text']:\n",
        "      tokens = tokenizer.encode(item['text'])\n",
        "      if len(tokens) < max_tokens:\n",
        "        filtered_eval_dataset.append(item)\n",
        "\n",
        "        count += 1\n",
        "      if count >= valid_size:\n",
        "        break\n",
        "\n",
        "  return filtered_train_dataset, filtered_eval_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sa8GkjdgTc5"
      },
      "outputs": [],
      "source": [
        "qa_train, qa_val = get_filtered_dataset(qa_dataset, MAX_TOKENS, DATASET_SIZE)\n",
        "qa_train = Dataset.from_list(qa_train)\n",
        "qa_val = Dataset.from_list(qa_val)\n",
        "print(qa_train)\n",
        "print(qa_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og0eYgN-gdo4"
      },
      "outputs": [],
      "source": [
        "qa_val[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IAWIFt_6nya"
      },
      "outputs": [],
      "source": [
        "# set constants\n",
        "MAX_SEQUENCE_LENGTH = 350 # calculated from avg token lenght in dataset\n",
        "\n",
        "# Run name for saving the model in the hub\n",
        "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
        "PROJECT_RUN_NAME = f\"{project_name}-{RUN_NAME}\"\n",
        "HUB_MODEL_NAME = f\"{hf_username}/{PROJECT_RUN_NAME}\"\n",
        "\n",
        "# qlora hyper params\n",
        "LORA_R = 16 # Reduce LoRA rank (lower = less memory) , initial = 32\n",
        "LORA_ALPHA = 32 # Lower alpha , initial = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.05\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "# training hyper params\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 4  # Reduce batch size , initial = 4\n",
        "GRADIENT_ACCUMULATION_STEPS = 8  # Simulate batch size 8, initial = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "STEPS = 20\n",
        "SAVE_STEPS = 2000\n",
        "LOG_TO_WANDB = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wJwH1MA-tyo"
      },
      "outputs": [],
      "source": [
        "response_template = \" [/INST] \"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbo_-tkbpICk"
      },
      "outputs": [],
      "source": [
        "lora_params = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_WWBQVSyPJu"
      },
      "outputs": [],
      "source": [
        "training_params = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    #eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if True else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True\n",
        "    #neftune_noise_alpha=5 # using NEFTune as describe in SFT Trainer docs for increased conversational quality\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaOIGnQ4jxVS"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsMERCCJj99j"
      },
      "outputs": [],
      "source": [
        "# clear gpu memory\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_max_memory_allocated()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for k-bit training (important for 4-bit)\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "lz825BwicfAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_params)"
      ],
      "metadata": {
        "id": "pp8vo6najkmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY_UzTxetRc5"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=qa_train,\n",
        "    eval_dataset=qa_val,\n",
        "    peft_config=lora_params,\n",
        "    args=training_params\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Ko4tyGJftW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ushOfJdwuMAZ"
      },
      "outputs": [],
      "source": [
        "#torch.cuda.empty_cache()\n",
        "trainer.train()\n",
        "trainer.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Model saved to HuggingFace as: {PROJECT_RUN_NAME}\")\n",
        "model.save_pretrained(PROJECT_RUN_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7QAra8y9v2Z"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "IGIlJtkHn3Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ed_medical-2025-05-12_02.31.47.zip ed_medical-2025-05-12_02.31.47"
      ],
      "metadata": {
        "id": "bmafejzxtwEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la"
      ],
      "metadata": {
        "id": "JeJNRxKawEhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp ed_medical-2025-05-12_02.31.47.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "fZLDcQeaxiL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop notebook and disconnect GPU after finishing above steps as this process can take several hours\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "RAC0SrTL8Wt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6sQTtZuSoT0"
      },
      "source": [
        "## 5. Evaluate the models - generate evaluation data\n",
        "At this step the fine-tuned model created in the previous step can be re-loaded without need to re-run step 4. Generated responses will be collected, in addition to the base model to use as a benchmark.\n",
        "\n",
        "The prompts to generate the responses are 50 samples from the **test** set which was not used for fine-tuning.\n",
        "\n",
        "The generated response for each model are then saved to HuggingFace hub for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define models saved to huggingface hub from previous step\n",
        "MODEL_A = \"digitalblue/ed_medical-2025-04-03_09.11.29\" # trained on 200 rows\n",
        "MODEL_B = \"digitalblue/ed_medical-2025-04-05_10.59.54\" # trained on 800 rows\n",
        "MODEL_C = \"digitalblue/ed_medical-2025-04-05_11.36.32\" # trained on 1600 rows\n",
        "MODEL_D = \"digitalblue/ed_medical-2025-04-06_10.40.29\" # trained on 1600 rows with NEFTune\n",
        "MODEL_E = \"digitalblue/ed_medical-2025-04-15_12.43.48\" # trained on 8000 rows of pubmedqa only w/ NEFTune\n",
        "MODEL_F = \"digitalblue/ed_medical-2025-05-05_23.51.25\" # trained on 9000 row, lower learing rate, 3 epochs\n",
        "MODEL_G = \"digitalblue/ed_medical-2025-05-08_01.23.37\" # trained on 9000 rows, NEFTune false\n",
        "MODEL_H = \"digitalblue/ed_medical-2025-05-11_04.05.52\" # trained on 900 row, with context, 3 epochs\n",
        "MODEL_I = \"digitalblue/ed_medical-2025-05-12_02.31.47\" # trained on 900 rows of MedDialog"
      ],
      "metadata": {
        "id": "BsSyJO-eOIbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: Only using model I for MedDialog"
      ],
      "metadata": {
        "id": "B5wxXHFM7W2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# quantisation config to use less memory when loading model\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "ibdFI1MXDOAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\")"
      ],
      "metadata": {
        "id": "9SZwXR5wOB8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialise tokenizer and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "7f062UDIQcVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfqi9mxLcCV_"
      },
      "outputs": [],
      "source": [
        "qa_test = load_dataset(\"digitalblue/ed_medical-med-dialog-ic\", split=\"test[:50]\") # first 50 from test set\n",
        "#qa_test = load_dataset(\"digitalblue/ed_medical-pubmedqa-artifical\", split=\"test[:50]\") # load only pubmedqa YES rows, test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ex4akcIcHh4"
      },
      "outputs": [],
      "source": [
        "# split into question and answer list\n",
        "qa_list = []\n",
        "for item in qa_test:\n",
        "  prompt = item['text']\n",
        "  question = re.search(r'\\[INST\\] (.*) \\[/INST\\]', prompt).group(1)\n",
        "  response = re.search(r'\\[/INST\\] (.*) \\</s\\>', prompt).group(1)\n",
        "  qa_list.append([question, response])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smzkASMLcfeK"
      },
      "outputs": [],
      "source": [
        "qa_list[40][0] # question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aHb4x2hfZFi"
      },
      "outputs": [],
      "source": [
        "prompt1 = \"You are a helpful personalised medical assistant. In your response do not include any personal names and keep your answer professional and concise.\"\n",
        "#prompt2 = \"You are a helpful personalised medical assistant. Provide a friendly personal response to the patients medical question within a paragraph, provide the necessary details. Follow up it they need more information\"\n",
        "\n",
        "def get_model_responses(model_x, qa_list):\n",
        "  model_responses = []\n",
        "  for item in qa_list:\n",
        "    question = item[0]\n",
        "    # print(item)\n",
        "    # print(question)\n",
        "    # print(\"-----\")\n",
        "    messages = [\n",
        "      {\"role\": \"system\", \"content\": prompt1},\n",
        "      {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model_x.generate(inputs, max_new_tokens=250, temperature=0.1)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "    answer = response.split('[/INST] ')[1]  # get text after /INST token\n",
        "    answer = answer.replace('</s>', '') # remove trailing special token if present\n",
        "    model_responses.append(answer)\n",
        "  return model_responses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_base_responses_med_dialog = get_model_responses(model_base, qa_list)\n",
        "prompt_base_med_dialog_dataset = Dataset.from_dict({\"text\": model_base_responses_med_dialog})\n",
        "prompt_base_med_dialog_dataset.push_to_hub(f\"{hf_username}/model_base_responses_med_dialog\", private=True)"
      ],
      "metadata": {
        "id": "sX8ZCqBJ0JmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_i = PeftModel.from_pretrained(model_base, MODEL_I)"
      ],
      "metadata": {
        "id": "9szJl0P2PlpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_base.lm_head.weight[1000, :20])\n",
        "print(model_i.lm_head.weight[1000, :20])"
      ],
      "metadata": {
        "id": "U__jRWbmRBXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_i_responses = get_model_responses(model_i, qa_list)\n",
        "prompt_i_dataset = Dataset.from_dict({\"text\": model_i_responses})\n",
        "prompt_i_dataset.push_to_hub(f\"{hf_username}/model_i_responses\", private=True)"
      ],
      "metadata": {
        "id": "8m5IgtC-PwvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stop notebook and disconnect GPU after finishing above steps as this process can take several hours\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "Ub6-RR3uTnVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluate fine tuned models - get metrics from generated responses\n",
        "In this final step, the generated response can be reloaded from HuggingFace to evaluate and calculate metrics against the expected responses in **test** dataset."
      ],
      "metadata": {
        "id": "KwqCV9psQNpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_test = load_dataset(\"digitalblue/ed_medical-med-dialog-ic\", split=\"test[:50]\") # load only pubmedqa YES rows, test set"
      ],
      "metadata": {
        "id": "gfyOLbFnQVqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_list = []\n",
        "for item in qa_test:\n",
        "  prompt = item['text']\n",
        "  question = re.search(r'\\[INST\\] (.*) \\[/INST\\]', prompt).group(1)\n",
        "  response = re.search(r'\\[/INST\\] (.*) \\</s\\>', prompt).group(1)\n",
        "  qa_list.append([question, response])"
      ],
      "metadata": {
        "id": "cY3cjfLbQsJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_list[40][1] # index 1 is dataset real response"
      ],
      "metadata": {
        "id": "mAGTjeruQwJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_references = [item[1] for item in qa_list] # get list of reference responses"
      ],
      "metadata": {
        "id": "HVQzvtF1VIz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load saved response data to evaluate\n",
        "model_base_med_dialog_dataset = load_dataset(\"digitalblue/model_base_responses_med_dialog\")\n",
        "model_i_dataset = load_dataset(\"digitalblue/model_i_responses\")"
      ],
      "metadata": {
        "id": "526aY-AERByq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to list\n",
        "base_med_dialog_response_list = model_base_med_dialog_dataset['train']['text'][:50]\n",
        "i_response_list = model_i_dataset['train']['text']"
      ],
      "metadata": {
        "id": "OA2TvC2CR9an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_rouge_metric(references, predictions):\n",
        "  rouge = evaluate.load('rouge')\n",
        "  results = rouge.compute(\n",
        "    predictions=predictions,\n",
        "    references=references\n",
        "  )\n",
        "  return results\n",
        "\n",
        "def calc_bleu_metric(references, predictions):\n",
        "  bleu = evaluate.load('bleu')\n",
        "  results = bleu.compute(\n",
        "    predictions=predictions,\n",
        "    references=references\n",
        "  )\n",
        "  return results\n",
        "\n",
        "def calc_meteor_metric(references, predictions):\n",
        "  meteor = evaluate.load('meteor')\n",
        "  results = meteor.compute(\n",
        "    predictions=predictions,\n",
        "    references=[[ref] for ref in references]\n",
        "  )\n",
        "  return results\n",
        "\n",
        "def calc_bert_score(references, predictions):\n",
        "  bert_scorer = BERTScorer(model_type='bert-base-uncased')\n",
        "  P = []\n",
        "  R = []\n",
        "  F1 = []\n",
        "\n",
        "  for i in range(len(predictions)):\n",
        "    p_i, r_i, f1_i = bert_scorer.score([predictions[i]], [references[i]])\n",
        "    P.append(p_i)\n",
        "    R.append(r_i)\n",
        "    F1.append(f1_i)\n",
        "  results = {\n",
        "    'P': np.mean(P),\n",
        "    'R': np.mean(R),\n",
        "    'F1': np.mean(F1)\n",
        "  }\n",
        "  return results"
      ],
      "metadata": {
        "id": "yzfSECeMTK67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_base_med_dialog = calc_rouge_metric(qa_references, base_med_dialog_response_list)\n",
        "rouge_i = calc_rouge_metric(qa_references, i_response_list)"
      ],
      "metadata": {
        "id": "7CnxpXVMU-kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rouge_base_med_dialog)\n",
        "print(rouge_i)"
      ],
      "metadata": {
        "id": "R2dEf1Mw5tbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot rouge scores\n",
        "rouge_dict = {\n",
        "    'rouge1': (rouge_base_med_dialog['rouge1'], rouge_i['rouge1']),\n",
        "    'rouge2': (rouge_base_med_dialog['rouge2'], rouge_i['rouge2']),\n",
        "    'rougeL': (rouge_base_med_dialog['rougeL'], rouge_i['rougeL']),\n",
        "    'rougeLsum': (rouge_base_med_dialog['rougeLsum'], rouge_i['rougeLsum'])\n",
        "}\n",
        "model_labels = ('Base', 'Model I')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in rouge_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Rouge Score')\n",
        "ax.set_title('Rouge Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6rBYsoNN4_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_base_med_dialog = calc_bleu_metric(qa_references, base_med_dialog_response_list)\n",
        "bleu_i = calc_bleu_metric(qa_references, i_response_list)"
      ],
      "metadata": {
        "id": "LvagAtBTTJam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bleu_base_med_dialog)\n",
        "print(bleu_i)"
      ],
      "metadata": {
        "id": "E_P5x81pTTq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot bleu scores\n",
        "bleu_dict = {\n",
        "    'bleu': (bleu_base_med_dialog['bleu'], bleu_i['bleu']),\n",
        "    'brevity_penalty': (bleu_base_med_dialog['brevity_penalty'], bleu_i['brevity_penalty'])\n",
        "}\n",
        "\n",
        "model_labels = ('Base', 'Model I')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in bleu_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Bleu Score')\n",
        "ax.set_title('Bleu Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VrdOyoXrUojV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meteor_base_med_dialog = calc_meteor_metric(qa_references, base_med_dialog_response_list)\n",
        "meteor_i = calc_meteor_metric(qa_references, i_response_list)"
      ],
      "metadata": {
        "id": "c1Hq-mzZTiSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(meteor_base_med_dialog)\n",
        "print(meteor_i)"
      ],
      "metadata": {
        "id": "DjJmRWA6T5Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot meteor scores\n",
        "meteor_dict = {\n",
        "    'meteor': (meteor_base_med_dialog['meteor'], meteor_i['meteor'])\n",
        "}\n",
        "model_labels = ('Base', 'Model I')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in meteor_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Meteor Score')\n",
        "ax.set_title('Meteor Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p0MZQuIYW738"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score_base_med_dialog = calc_bert_score(qa_references, base_med_dialog_response_list)\n",
        "bert_score_i = calc_bert_score(qa_references, i_response_list)"
      ],
      "metadata": {
        "id": "PX2qUlOaUBBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bert_score_base_med_dialog)\n",
        "print(bert_score_i)"
      ],
      "metadata": {
        "id": "tM-k91NhUM7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot bert scores\n",
        "bert_score_dict = {\n",
        "    'P': (bert_score_base_med_dialog['P'], bert_score_i['P']),\n",
        "    'R': (bert_score_base_med_dialog['R'], bert_score_i['R']),\n",
        "    'F1': (bert_score_base_med_dialog['F1'], bert_score_i['F1'])\n",
        "}\n",
        "model_labels = ('Base', 'Model I')\n",
        "\n",
        "x = np.arange(len(model_labels))\n",
        "width = 0.2\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(layout='constrained')\n",
        "\n",
        "for attribute, measurement in bert_score_dict.items():\n",
        "    offset = width * multiplier\n",
        "    msr = np.round(measurement, 4) # round to 4 decimal places for plot\n",
        "    rects = ax.bar(x + offset, msr, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=1)\n",
        "    multiplier += 1\n",
        "\n",
        "ax.set_ylabel('Bert Score')\n",
        "ax.set_title('Bert Score Comparison')\n",
        "ax.set_xticks(x + width, model_labels)\n",
        "ax.legend(loc='upper left', ncols=4)\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rKJ15yvHYFoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# human evaluation\n",
        "def print_response(QA_NUM):\n",
        "  for i in range(QA_NUM):\n",
        "    print('-----------------------------------------')\n",
        "    print('\\nTest question: ' + qa_list[i][0])\n",
        "    print('\\nTest answer: ' + qa_list[i][1])\n",
        "    print('\\nBase: ' + base_med_dialog_response_list[i])\n",
        "    print('\\nModel I: ' + i_response_list[i])\n",
        "\n",
        "print_response(5)"
      ],
      "metadata": {
        "id": "ilTsBxeBAFD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluation Summary\n",
        "Changing the dataset to MedDialog IC shows that the fine-tuned Model_I performs better than the base model.\n",
        "\n",
        "On human inspection the Llama 2 base model provides a more conversational response, however the fine-tuned model response aligns better with the dataset, indicating that it has learned from the training.\n",
        "\n",
        "This suggests that the base Llama model was not trained on MedDialog dataset, whereas PubMedQA dataset may have been used to train Llama 2.\n",
        "\n",
        "Next steps:\n",
        "* Continue to experiment with further hyperparameter tuning\n",
        "* Increase volume of training data\n",
        "* Adjust system prompt, and model parameters on inference\n",
        "* Improve the pre-processing of the dataset, to provide a more useful answer via the Chatbot"
      ],
      "metadata": {
        "id": "A__kV9qaZ-lF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mAaeQ9-vdDsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install necessary packages for printing to pdf\n",
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc\n",
        "!pip install nbconvert"
      ],
      "metadata": {
        "id": "s1JQSJSJAXf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYqjD3cGk3rj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Kh2len0SEWYqzObuw3YszHRIkeLe1zts",
      "authorship_tag": "ABX9TyOFnEqtQaq6gap28QGJeVy1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}