{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DataBytes-Organisation/Fine-Tuning-LLMs-for-Enterprise-Applications/blob/ed_branch/Ed_LLM_FineTuning_Chatbot_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUb2oSz45XV4"
      },
      "source": [
        "# Personalized Healthcare QA System (Chatbot) - Chatbot UI\n",
        "#### Ed:215279167\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C8_1rPk-OkG"
      },
      "source": [
        "---\n",
        "## 1. Import libraries and model for Chatbot UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0NvJyuYeGBJ"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets requests bitsandbytes accelerate peft trl sentencepiece wandb transformers evaluate rouge_score bert-score gradio langchain-huggingface\n",
        "!pip install requests bitsandbytes wandb transformers gradio langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIeIbt2M9vut"
      },
      "source": [
        "### IMPORTANT: Restart Colab runtime after PIP install!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxw-QITQ9iW0"
      },
      "source": [
        "## 2. Implement base model and imports\n",
        "First step is to import all necessary libraries and implement base model, to validate it can generate a response from prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqaBKB9e4Vhe"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    logging,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model, TaskType\n",
        "import torch\n",
        "import wandb\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "# for chatbot UI\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4wnNrTyAJKX"
      },
      "outputs": [],
      "source": [
        "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "#model_name = \"digitalblue/model_e_merge_v2\"\n",
        "project_name = \"ed_medical\"\n",
        "hf_username = \"digitalblue\"\n",
        "\n",
        "# define peft adapters saved to huggingface hub\n",
        "MODEL_A = \"digitalblue/ed_medical-2025-04-03_09.11.29\" # trained on 200 rows\n",
        "MODEL_B = \"digitalblue/ed_medical-2025-04-05_10.59.54\" # trained on 800 rows\n",
        "MODEL_C = \"digitalblue/ed_medical-2025-04-05_11.36.32\" # trained on 1600 rows\n",
        "MODEL_D = \"digitalblue/ed_medical-2025-04-06_10.40.29\" # trained on 1600 rows with NEFTune\n",
        "MODEL_E = \"digitalblue/ed_medical-2025-04-15_12.43.48\" # trained on 8000 rows of pubmedqa only w/ NEFTune\n",
        "MODEL_F = \"digitalblue/ed_medical-2025-05-05_23.51.25\" # trained on 9000 row, lower learing rate, 3 epochs\n",
        "MODEL_G = \"digitalblue/ed_medical-2025-05-08_01.23.37\" # trained on 9000 rows, NEFTune false\n",
        "MODEL_H = \"digitalblue/ed_medical-2025-05-11_04.05.52\" # trained on 900 row, with context, 3 epochs\n",
        "MODEL_I = \"digitalblue/ed_medical-2025-05-12_02.31.47\" # trained on 900 rows of MedDialog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8r6Ypk6_x0S"
      },
      "outputs": [],
      "source": [
        "# log into hugging face and wandb\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)\n",
        "\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = \"ed_medical\"\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if True else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUQQz387Y6Ph"
      },
      "outputs": [],
      "source": [
        "# quantisation config to use less memory when loading model\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BTN1xwNb9OT"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogYrGfVTcVyR"
      },
      "outputs": [],
      "source": [
        "# initialise tokenizer and pipeline\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, token=hf_token)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8YnHIfVecat"
      },
      "outputs": [],
      "source": [
        "#model_g = PeftModel.from_pretrained(model, MODEL_G)\n",
        "model_i = PeftModel.from_pretrained(model, MODEL_I)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVJlB4AsdQ6q"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"You are a helpful personalised medical assistant. In your response do not include any personal names and ensure it is detailed. Do not say I have gone through your report or query. Indicate that you understand the questions with medical information.\"\n",
        "\n",
        "def get_model_response(model_x, prompt, chat_history):\n",
        "\n",
        "  messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "  ]\n",
        "  for turn in chat_history:\n",
        "    print(f\"turn={turn}\")\n",
        "    messages.append(turn)\n",
        "\n",
        "  messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "  print(f\"messages={messages}\")\n",
        "\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "  outputs = model_x.generate(inputs, max_new_tokens=512, temperature=0.1)\n",
        "  response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "  print(f\"response={response}\")\n",
        "  answer = response.split(\"[/INST]\")[-1].strip()\n",
        "  answer = answer.replace('</s>', '') # remove trailing special token if present\n",
        "  print(f\"answer={answer}\")\n",
        "  return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4fLCDWSfj6U"
      },
      "outputs": [],
      "source": [
        "get_model_response(model_i, \"Hi, does utility of preliminary bronchoalveolar lavage result in suspected ventilator-associated pneumonia?\", [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTxw5TzZdfS7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def respond(message, chat_history):\n",
        "  try:\n",
        "    # Format prompt for Llama 2\n",
        "    print(f\"chat_history={chat_history}\")\n",
        "    print(f\"message={message}\")\n",
        "\n",
        "    # Get model output\n",
        "    model_output = get_model_response(model_i, message, chat_history)\n",
        "    print(f\"model_output = {model_output}\")\n",
        "    print(\"--------\")\n",
        "\n",
        "    return model_output\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error in respond(): {str(e)}\")\n",
        "    return \"Error occurred\", chat_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcZz_RQU-N0l"
      },
      "outputs": [],
      "source": [
        "force_dark_mode = \"\"\"\n",
        "function refresh() {\n",
        "    const url = new URL(window.location);\n",
        "    if (url.searchParams.get('__theme') !== 'dark') {\n",
        "        url.searchParams.set('__theme', 'dark');\n",
        "        window.location.href = url.href;\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "chatbot_ui = gr.ChatInterface(\n",
        "    fn=respond,\n",
        "    type=\"messages\",\n",
        "    flagging_mode=\"never\",\n",
        "    js=force_dark_mode)\n",
        "\n",
        "\n",
        "chatbot_ui.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsMERCCJj99j"
      },
      "outputs": [],
      "source": [
        "# clear gpu memory\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_max_memory_allocated()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1Kh2len0SEWYqzObuw3YszHRIkeLe1zts",
      "authorship_tag": "ABX9TyOoWDt6s0xeVwZCHo42r5DQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}