{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f9e8430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fd848822b344adbf771b720a3a6400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:3405: UserWarning: Attempting to save a model with offloaded modules. Ensure that unallocated cpu memory exceeds the `shard_size` (5GB default)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ed04d73a2446aca9a954c4ba6dc82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./mistral-combined-finetuned-weights\\\\tokenizer_config.json',\n",
       " './mistral-combined-finetuned-weights\\\\special_tokens_map.json',\n",
       " './mistral-combined-finetuned-weights\\\\tokenizer.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "base_model_path = \"./Mistral-7B-v0.1\"\n",
    "finetuned_model_path = \"./mistral-risk-finetuned-final\"\n",
    "output_path = \"./mistral-combined-finetuned-weights\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"./offload\"\n",
    ")\n",
    "\n",
    "# Load and merge PEFT (LoRA) weights\n",
    "model = PeftModel.from_pretrained(model, finetuned_model_path, offload_folder=\"./offload\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "model.save_pretrained(output_path)\n",
    "tokenizer.save_pretrained(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e2c4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
