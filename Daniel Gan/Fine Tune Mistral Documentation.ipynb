{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9537d0e0",
   "metadata": {},
   "source": [
    "This use case fine-tunes the Mistral-7B model on the Gretel Financial Risk Analysis dataset using parameter-efficient fine-tuning (PEFT) via LoRA (Low-Rank Adaptation). The end goal is a domain-adapted model that can perform structured financial risk assessment in natural language using a constrained, cost-effective approach suitable for deployment and continual learning in the form of feature extraction and test summurisation. The use case proritizes the feasibility of local-run approach that utilizes consumer grade GPU to deliver real world solution.\n",
    "\n",
    "Task: Sequence generation for structured financial risk extraction.\n",
    "\n",
    "Setting: Resource-efficient fine-tuning using quantized models and LoRA adapters.\n",
    "\n",
    "Challenges:\n",
    "\n",
    "        Domain shift from general pretraining to financial documents.\n",
    "\n",
    "        Memory constraints from 7B parameters on consumer hardware.\n",
    "\n",
    "        Double NLP tasks: feature extraction + text summarisation       \n",
    "\n",
    "This use case is originally proposed and authored by Daniel Gan, https://github.com/FinalTwilite.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc31d0",
   "metadata": {},
   "source": [
    "Mistral-7B Financial Risk Analysis Fine-Tuning\n",
    "\n",
    "\n",
    "This notebook documents the process of fine-tuning the Mistral-7B large language model for specialized financial risk analysis. We employ parameter-efficient techniques including Low-Rank Adaptation (LoRA) and quantization to create a model capable of identifying financial risks in documents and providing structured assessments.\n",
    "The approach leverages state-of-the-art techniques to enable fine-tuning of a 7B parameter model with modest computational resources, producing a specialized model for financial domain applications.\n",
    "Environment Setup and Dependencies\n",
    "We begin by importing necessary libraries that provide the foundation for our fine-tuning process. The key libraries include:\n",
    "\n",
    "transformers: Hugging Face's library providing access to pre-trained models and training utilities\n",
    "peft: Parameter-Efficient Fine-Tuning library that implements LoRA and other PEFT methods\n",
    "datasets: Hugging Face's dataset handling library for efficient data loading and processing\n",
    "torch: PyTorch for deep learning operations\n",
    "BitsAndBytesConfig: Specialized quantization configuration for reducing memory requirements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a261abc",
   "metadata": {},
   "source": [
    " Base Model: Mistral-7B\n",
    "Chosen for its open-access, competitive performance, and architectural simplicity at consumer grade hardware. \n",
    "\n",
    "Mistral is a dense decoder-only transformer, similar to LLaMA2, but with architectural improvements in attention efficiency and context handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2f59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b3dadc",
   "metadata": {},
   "source": [
    "Dataset Selection and Analysis\n",
    "\n",
    "For this fine-tuning task, we use the specialized gretelai/gretel-financial-risk-analysis-v1 dataset from the Hugging Face Hub. This dataset is particularly valuable because it contains:\n",
    "\n",
    "Financial documents and texts with varying levels of risk indicators\n",
    "Expert-crafted risk assessments in structured JSON format\n",
    "Diverse financial contexts and scenarios\n",
    "Pre-split train and test sets for proper evaluation\n",
    "\n",
    "The dataset enables the model to learn patterns of financial risk indicators and how to express them in a standardized, structured format suitable for further automated processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52185f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the financial risk analysis dataset from HuggingFace Hub\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"gretelai/gretel-financial-risk-analysis-v1\")\n",
    "\n",
    "# Convert to pandas DataFrames for easier inspection (optional)\n",
    "train_df = dataset[\"train\"].to_pandas()\n",
    "val_df = dataset[\"test\"].to_pandas()\n",
    "\n",
    "# Dataset exploration (not in original code but useful for understanding)\n",
    "# print(f\"Training examples: {len(train_df)}\")\n",
    "# print(f\"Validation examples: {len(val_df)}\")\n",
    "# print(f\"Sample input:\\n{train_df['input'][0][:500]}...\")\n",
    "# print(f\"Sample output:\\n{train_df['output'][0][:500]}...\")\n",
    "# otherwise join the huggingface to \"see\" more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a538f",
   "metadata": {},
   "source": [
    "Tokenizer Configuration\n",
    "\n",
    "The tokenizer is responsible for converting raw text into tokens that the model can process. Mistral-7B uses a specialized tokenizer based on BPE (Byte-Pair Encoding) that must be properly configured to handle our specific prompt format and task requirements.\n",
    "Key tokenizer considerations:\n",
    "\n",
    "We ensure the pad token is set to match the EOS (End of Sequence) token, as this is standard for Mistral\n",
    "The tokenizer must properly handle the chat format we'll use, including system messages and instruction formatting\n",
    "We load it from the same path as our base model to ensure compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39fe315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Mistral tokenizer from local path\n",
    "print(\"Loading tokenizer...\")\n",
    "model_path = \"./Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Mistral uses eos token as pad token - this is crucial for proper formatting\n",
    "# Without this, padding would use a different token and confuse the model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Note: We could inspect tokenizer properties here\n",
    "# print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "# print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "# print(f\"Special tokens: {tokenizer.all_special_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f1242",
   "metadata": {},
   "source": [
    "Quantization Strategy\n",
    "\n",
    "Quantization is a technique to reduce the memory footprint and computational requirements of large language models by representing weights with lower precision. We use 4-bit quantization through the BitsAndBytesConfig to dramatically reduce the VRAM requirements for fine-tuning.\n",
    "This approach enables us to fine-tune a 7B parameter model on hardware with limited GPU memory (e.g., a single consumer GPU with 16-24GB VRAM) that would otherwise be impossible with full precision training.\n",
    "Key quantization parameters:\n",
    "\n",
    "4-bit precision for weights (1/8 the memory of FP32)\n",
    "Double quantization for further memory savings\n",
    "NF4 data type balancing precision and efficiency\n",
    "Float16 compute dtype for faster operations while maintaining reasonable precision\n",
    "\n",
    "#Author used RTX4090, if stronger hardware is available, consequencetly the expansion of this qunatization performance, hyperparameter, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaeeb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization to reduce memory requirements\n",
    "# Without quantization, a 7B model would require ~28GB in FP16 and ~56GB in FP32\n",
    "print(\"Setting up quantization...\")\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,              # Use 4-bit quantization for weights (vs FP16/FP32)\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute operations in float16 precision\n",
    "    bnb_4bit_use_double_quant=True, # Nested quantization: quantize the quantized weights\n",
    "    bnb_4bit_quant_type=\"nf4\"       # NF4 format: normalized float 4 - balances quality & compression\n",
    ")\n",
    "\n",
    "# Load the base model with quantization settings applied\n",
    "print(\"Loading model with quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=quantization_config,  # Apply our quantization settings\n",
    "    device_map=\"auto\",              # Automatically optimize model placement across available devices\n",
    "    torch_dtype=torch.float16       # Use half-precision for non-quantized tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f54bc7b",
   "metadata": {},
   "source": [
    "Low-Rank Adaptation (LoRA) Implementation\n",
    "\n",
    "LoRA is a parameter-efficient fine-tuning technique that dramatically reduces the number of trainable parameters by adding small, trainable rank decomposition matrices to existing weights rather than modifying all model weights.\n",
    "This approach offers several significant advantages:\n",
    "\n",
    "       Memory efficiency: Only a small fraction of parameters need to be stored and updated       \n",
    "\n",
    "       Training speed: Fewer parameter updates means faster training       \n",
    "\n",
    "       Adaptability: Low-rank updates can effectively adapt pre-trained knowledge to specialized domains       \n",
    "\n",
    "       Stability: Limited parameter updates help prevent catastrophic forgetting and overfitting       \n",
    "\n",
    "We carefully configure LoRA for the Mistral architecture by:\n",
    "\n",
    "Targeting attention layers and MLP projections\n",
    "Using an appropriate rank (r=16) that balances expressivity and efficiency\n",
    "Applying proper scaling (alpha=32) and regularization (dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ffe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Low-Rank Adaptation (LoRA) for efficient fine-tuning\n",
    "# LoRA adds small trainable matrices to existing weights using the decomposition:\n",
    "# ΔW = A×B where A ∈ ℝ^(d×r), B ∈ ℝ^(r×k), and typically r << min(d,k)\n",
    "print(\"Configuring LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,   # Configure for causal (autoregressive) language modeling\n",
    "    r=16,                           # Rank of LoRA decomposition - higher means more capacity but more parameters\n",
    "    lora_alpha=32,                  # Scaling factor for LoRA updates (effectively α/r scaling for updates)\n",
    "    lora_dropout=0.5,               # Dropout probability for LoRA layers - important for regularization\n",
    "    \n",
    "    # Target specific modules in the Mistral architecture\n",
    "    # This is a critical choice that impacts performance and efficiency\n",
    "    target_modules=[\n",
    "        \"q_proj\",                   # Query projection in attention mechanism\n",
    "        \"k_proj\",                   # Key projection in attention mechanism\n",
    "        \"v_proj\",                   # Value projection in attention mechanism\n",
    "        \"o_proj\",                   # Output projection from attention mechanism\n",
    "        \"gate_proj\",                # Gate projection in MLP blocks (for SwiGLU activation)\n",
    "        \"up_proj\",                  # Up-projection in MLP blocks\n",
    "        \"down_proj\",                # Down-projection in MLP blocks\n",
    "    ],\n",
    "    bias=\"none\",                    # Do not train bias parameters to reduce overfitting risk\n",
    ")\n",
    "\n",
    "# Apply LoRA configuration to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print parameter statistics to verify our configuration\n",
    "# This shows what percentage of parameters will be trained vs frozen\n",
    "model.print_trainable_parameters()  # Expected output: trainable params << total params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620eee79",
   "metadata": {},
   "source": [
    "Data Formatting and Instruction Tuning Strategy\n",
    "\n",
    "Instruction fine-tuning requires careful formatting of prompts and responses to match the model's expected input pattern. For Mistral-7B, we use the chat template format with a system message, user instruction, and assistant response.\n",
    "Our approach:\n",
    "\n",
    "       Include a consistent system message defining the financial risk analysis role       \n",
    "       Format each example using Mistral's specific chat syntax: <s>[INST] prompt [/INST] response</s>       \n",
    "       Include raw inputs from financial documents and structured outputs with risk assessments       \n",
    "       Ensure proper tokenization, padding, and attention masking       \n",
    "\n",
    "The resulting training examples teach the model both:\n",
    "\n",
    "       How to understand financial documents and identify risks        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55c7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a clear system prompt that establishes the model's role and output format\n",
    "# This guides the model to produce structured, consistent risk analyses\n",
    "SYSTEM_MESSAGE = (\n",
    "    \"You are an expert financial risk analyst. Analyze the provided text for financial risks, \"\n",
    "    \"and output a structured assessment in JSON format including risk detection, specific risk flags, \"\n",
    "    \"financial exposure details, and analysis notes.\"\n",
    ")\n",
    "\n",
    "# Data preprocessing function to format examples according to Mistral's chat template\n",
    "def preprocess_function(examples):\n",
    "    formatted_prompts = []\n",
    "    for i in range(len(examples[\"input\"])):\n",
    "        # Format using Mistral's chat template with system message, instruction, and response\n",
    "        # Format: <s>[INST] <<SYS>>\\nsystem message\\n<</SYS>>\\n\\nuser input [/INST] model response</s>\n",
    "        prompt = f\"<s>[INST] <<SYS>>\\n{SYSTEM_MESSAGE}\\n<</SYS>>\\n\\n{examples['input'][i]} [/INST] {examples['output'][i]}</s>\"\n",
    "        formatted_prompts.append(prompt)\n",
    "    \n",
    "    # Tokenize all prompts with proper padding and truncation settings\n",
    "    tokenized = tokenizer(\n",
    "        formatted_prompts,\n",
    "        padding=\"max_length\",       # Pad all sequences to same length for batch processing\n",
    "        truncation=True,            # Truncate if exceeds maximum allowed length\n",
    "        max_length=512,             # Maximum sequence length (context window)\n",
    "        return_tensors=\"pt\"         # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # Prepare dataset in the format expected by the Trainer\n",
    "    # For causal LM, labels are identical to input_ids for teacher forcing\n",
    "    result = {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"input_ids\"].clone()  # Copy input_ids to labels\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Process training and validation datasets\n",
    "print(\"Processing training data...\")\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,                     # Process multiple examples at once for efficiency\n",
    "    batch_size=16,                    # Number of examples to process in each batch\n",
    "    remove_columns=dataset[\"train\"].column_names  # Remove original text columns after processing\n",
    ")\n",
    "\n",
    "print(\"Processing validation data...\")\n",
    "val_dataset = dataset[\"test\"].map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    remove_columns=dataset[\"test\"].column_names\n",
    ")\n",
    "\n",
    "# Optionally inspect the processed dataset (not in original code)\n",
    "# print(f\"Training example shape: {train_dataset[0]['input_ids'].shape}\")\n",
    "# print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "# print(f\"Number of validation examples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e912920",
   "metadata": {},
   "source": [
    "Training Configuration and Hyperparameter Selection\n",
    "\n",
    "Selecting appropriate training hyperparameters is critical for effective fine-tuning. Our configuration balances:\n",
    "\n",
    "       Memory constraints: Using small batch sizes with gradient accumulation       \n",
    "       Learning dynamics: Proper learning rate, schedule, and regularization       \n",
    "       Training efficiency: Optimal epochs, evaluation frequency, and checkpoint saving       \n",
    "       Model quality: Validation-based model selection and proper precision settings       \n",
    "\n",
    "Key hyperparameter choices:\n",
    "\n",
    "       Small per-device batch size (2) with gradient accumulation (16) simulating a larger batch of 32       \n",
    "       Moderate learning rate (2e-5) with cosine schedule and 10% warmup       \n",
    "       Strong weight decay (0.5) to prevent overfitting given the small dataset       \n",
    "       Mixed precision training (fp16) for memory efficiency       \n",
    "       Checkpoint management saving only the best 3 models       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d19072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the training process with carefully tuned hyperparameters\n",
    "# These settings are optimized for LoRA fine-tuning of Mistral-7B on consumer hardware\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral_risk_finetuned\",  # Directory to save checkpoints\n",
    "    \n",
    "    # Evaluation strategy\n",
    "    eval_strategy=\"epochs\",         # Evaluate after each epoch\n",
    "    eval_steps=100,                 # Evaluate every 100 steps (backup if epochs are very long)\n",
    "    \n",
    "    # Saving strategy\n",
    "    save_strategy=\"epochs\",         # Save model after each epoch\n",
    "    save_steps=100,                 # Save checkpoint every 100 steps (backup)\n",
    "    save_total_limit=3,             # Only keep 3 best checkpoints to save disk space\n",
    "    load_best_model_at_end=True,    # Load best model after training (based on eval metric)\n",
    "    \n",
    "    # Learning rate and schedule\n",
    "    learning_rate=2e-5,             # 2e-5 is effective for LoRA fine-tuning\n",
    "    lr_scheduler_type=\"cosine\",     # Cosine schedule works well for LLM fine-tuning\n",
    "    warmup_ratio=0.1,               # Warm up learning rate for 10% of training steps\n",
    "    \n",
    "    # Batch size configuration - critical for memory management\n",
    "    per_device_train_batch_size=2,  # Small batch size due to memory constraints\n",
    "    per_device_eval_batch_size=2,   # Small batch size for evaluation\n",
    "    gradient_accumulation_steps=16, # Accumulate gradients over 16 steps (effective batch size = 32)\n",
    "    \n",
    "    # Training length\n",
    "    num_train_epochs=6,             # Total number of training epochs\n",
    "    \n",
    "    # Regularization\n",
    "    weight_decay=0.5,               # Strong L2 regularization to prevent overfitting\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=True,                      # Use mixed precision training for memory efficiency\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=10,               # Log metrics every 10 steps\n",
    "    report_to=\"tensorboard\",        # Log metrics to TensorBoard for visualization\n",
    "    \n",
    "    # Model selection\n",
    "    metric_for_best_model=\"eval_loss\",  # Select best model based on validation loss\n",
    "    greater_is_better=False,        # For loss, lower values are better\n",
    ")\n",
    "\n",
    "# Data collator handles batching and formatting for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,            # Use our configured tokenizer\n",
    "    mlm=False                       # Use causal language modelling (not masked LM)\n",
    ")\n",
    "\n",
    "# Initialize and configure the HuggingFace Trainer\n",
    "# The Trainer handles the training loop, evaluation, and checkpointing\n",
    "trainer = Trainer(\n",
    "    model=model,                    # Our LoRA-adapted model\n",
    "    args=training_args,             # Training configuration\n",
    "    train_dataset=train_dataset,    # Processed training data\n",
    "    eval_dataset=val_dataset,       # Processed validation data\n",
    "    data_collator=data_collator,    # Data formatting utility\n",
    ")\n",
    "\n",
    "# Start the fine-tuning process\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7c39eb",
   "metadata": {},
   "source": [
    "Model Saving and Deployment Strategy\n",
    "\n",
    "After fine-tuning, we need to properly save the model artifacts for future use. With LoRA, we have two main options:\n",
    "\n",
    "Save adapter weights only: Much smaller files, but requires loading the base model separately\n",
    "Create a merged model: Combines base model and adaptations for simpler deployment\n",
    "\n",
    "We implement both approaches to provide flexibility for different deployment scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54887ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned LoRA adapter weights\n",
    "# These are much smaller than the full model (typically <100MB vs 13GB+)\n",
    "save_directory = \"./mistral-risk-finetuned-final\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save the LoRA adapter weights and tokenizer configuration\n",
    "model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "print(f\"Model adapter and tokenizer saved to {save_directory}\")\n",
    "\n",
    "# Optional: Create a merged model for easier inference\n",
    "# This combines the base model weights with the LoRA adaptations\n",
    "print(\"Creating merged model for easier inference...\")\n",
    "merged_model_path = \"./mistral-risk-merged\"\n",
    "os.makedirs(merged_model_path, exist_ok=True)\n",
    "\n",
    "# Note: The actual merging code would be implemented here\n",
    "# Typically using model.merge_and_unload() from PEFT library\n",
    "# This would create a standalone model that doesn't require separate adapter loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1f11f",
   "metadata": {},
   "source": [
    "Inference and Model Usage\n",
    "\n",
    "To effectively use our fine-tuned model for financial risk analysis, we implement a streamlined inference pipeline. \n",
    "This approach loads the model with its LoRA adapters and provides a straightforward function for analyzing financial documents:\n",
    "\n",
    "\n",
    "\n",
    "       Model Loading: We load the base model and apply LoRA weights separately for maximum flexibility       \n",
    "       Memory Management: We use device mapping and offloading capabilities to handle memory constraints      \n",
    "        \n",
    "Generation Parameters:\n",
    "\n",
    "       Temperature of 0.7 balances creativity with accuracy       \n",
    "       Top-p sampling of 0.95 ensures reasonable output diversity       \n",
    "       Maximum token generation of 512 allows for comprehensive analysis       \n",
    "\n",
    "\n",
    "Performance Measurement: Tracking generation time helps optimize deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ad66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import time\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "base_model_path = \"./Mistral-7B-v0.1\"             # Base model\n",
    "finetuned_model_path = \"./mistral-risk-finetuned-final\"    # Fine-tuned weights\n",
    "\n",
    "# Initialize tokenizer from base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure proper padding token\n",
    "\n",
    "# Load the base model first\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,         # Use half-precision for efficiency\n",
    "    device_map=\"auto\",                 # Automatically distribute across available GPUs\n",
    "    offload_folder=\"./offload\"         # Optional disk offloading for large models\n",
    ")\n",
    "\n",
    "# Apply fine-tuned LoRA weights to the base model\n",
    "model = PeftModel.from_pretrained(model, finetuned_model_path)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define the inference function for financial risk analysis\n",
    "def generate_analysis(input_data: str, max_new_tokens=512, temperature=0.7, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Generate financial risk analysis from input text.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Financial text to analyze\n",
    "        max_new_tokens: Maximum number of tokens to generate\n",
    "        temperature: Controls randomness (lower = more deterministic)\n",
    "        top_p: Nucleus sampling parameter (higher = more diverse)\n",
    "        \n",
    "    Returns:\n",
    "        Generated risk analysis text\n",
    "    \"\"\"\n",
    "    # Create prompt with system instructions\n",
    "    prompt = f\"You are an expert financial risk analyst. Analyze the provided text for financial risks, and output a structured assessment in JSON format including risk detection, specific risk flags, financial exposure details, and analysis notes. {input_data}\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate output with timing\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        end = time.time()\n",
    "    \n",
    "    # Decode and return the result\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"\\n Generation time: {end - start:.2f} seconds\")\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example financial text for analysis\n",
    "    input_text = \"\"\"\n",
    "    The Company has entered into a five-year contract to purchase raw materials\n",
    "    from a single supplier in a volatile market. The contract requires minimum\n",
    "    purchases of $10M annually with no cancellation clause. Recent market analysis\n",
    "    suggests potential price fluctuations of up to 40% in the next year.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate and display the analysis\n",
    "    output = generate_analysis(input_text)\n",
    "    print(\"\\n Generated Analysis:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f8c1b",
   "metadata": {},
   "source": [
    "LLM Compliance Evaluation Framework\n",
    "\n",
    "This document provides comprehensive documentation for a framework designed to evaluate language model compliance using a multi-metric approach. The framework systematically evaluates model responses against predefined test prompts using established NLP evaluation metrics.\n",
    "Overview\n",
    "\n",
    "The framework performs the following operations:\n",
    "\n",
    "       Loads a causal language model (LLM) and tokenizer       \n",
    "       Iterates through predefined compliance-focused test prompts       \n",
    "       Generates model responses for each prompt       \n",
    "       Computes multiple evaluation metrics for each response       \n",
    "       alculates performance statistics across multiple test runs       \n",
    "       Exports results to a structured document format       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b40f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP evaluation packages\n",
    "import evaluate\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Hugging Face components\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Document handling\n",
    "from docx import Document\n",
    "\n",
    "# Load evaluation metrics using the evaluate package from Hugging Face\n",
    "rouge = evaluate.load(\"rouge\")  # Text summarization evaluation metric\n",
    "bleu = evaluate.load(\"bleu\")    # Machine translation evaluation metric\n",
    "\n",
    "# 100 diverse test prompts for a compliance-focused LLM\n",
    "# These prompts have been omitted for brevity in the code but would typically\n",
    "# contain a variety of scenarios to test model compliance across different domains\n",
    "prompts = [\n",
    "    \"What are the key environmental risks highlighted in this report?\",\n",
    "    \"Summarize key cybersecurity vulnerabilities in the filing.\",\n",
    "    \"Assess financial statement accuracy based on disclosed risks.\",\n",
    "    \"Identify any potential conflicts of interest in governance structures.\",\n",
    "    \"What are the tax risks identified in the latest filing?\",\n",
    "    \"Highlight any liquidity risks mentioned in this filing.\",\n",
    "    \"Evaluate compliance with international anti-bribery laws.\",\n",
    "    \"Analyze the impact of market risk factors disclosed in this report.\",\n",
    "    \"What are the key risks identified in this company’s supply chain?\",\n",
    "    \"Summarize risk factors from the latest 10-Q filing.\",\n",
    "    \"Identify any references to potential violations of the Foreign Corrupt Practices Act (FCPA).\",\n",
    "    \"What governance issues are raised in this proxy statement?\",\n",
    "    \"Detect any conflicts between financial projections and risk disclosures.\",\n",
    "    \"Identify operational inefficiencies highlighted in the filing.\",\n",
    "    \"Summarize management's response to identified risks.\",\n",
    "    \"What mitigation strategies are proposed for identified risks?\",\n",
    "    \"Review the risk management framework outlined in the document.\",\n",
    "    \"Analyze potential strategic risks identified in the report.\",\n",
    "    \"Highlight reputational risks mentioned in the document.\",\n",
    "    \"Identify regulatory penalties or fines discussed in the report.\",\n",
    "    \"Detect any signs of financial misreporting in the filing.\",\n",
    "    \"Analyze the company’s risk appetite based on the disclosed risks.\",\n",
    "    \"Identify political risks related to operations in foreign countries.\",\n",
    "    \"Assess the adequacy of the company’s disaster recovery plans.\",\n",
    "    \"What are the sustainability risks identified in this report?\",\n",
    "    \"Summarize legal proceedings related to risk in this filing.\",\n",
    "    \"Identify insurance coverage gaps discussed in the filing.\",\n",
    "    \"Detect any potential issues with intellectual property management.\",\n",
    "    \"Highlight key fraud risks disclosed in the document.\",\n",
    "    \"Summarize the company’s risk tolerance as outlined in the filing.\",\n",
    "    \"Assess the company’s risk diversification strategy.\",\n",
    "    \"Review how the company plans to handle potential supply chain disruptions.\",\n",
    "    \"Identify reputational risks related to the company’s brand.\",\n",
    "    \"Analyze risk exposure from foreign exchange fluctuations.\",\n",
    "    \"Summarize any risks related to customer concentration.\",\n",
    "    \"Detect conflicts of interest in the company’s executive compensation plan.\",\n",
    "    \"Identify risks related to mergers and acquisitions in the filing.\",\n",
    "    \"Analyze whether the company has adequate legal compliance programs.\",\n",
    "    \"Review how the company addresses regulatory changes in this document.\",\n",
    "    \"Summarize the company’s risk management priorities for the next year.\",\n",
    "    \"Assess risks related to changes in government policy.\",\n",
    "    \"Identify any material weaknesses in internal controls.\",\n",
    "    \"Evaluate the company’s risk management performance over time.\",\n",
    "    \"Highlight risks associated with operational outsourcing.\",\n",
    "    \"Assess the financial impact of risk events disclosed in the filing.\",\n",
    "    \"Summarize risks related to the company’s digital transformation efforts.\",\n",
    "    \"Identify key social risks disclosed in the filing.\",\n",
    "    \"What are the potential risks associated with the company’s new product launch?\",\n",
    "    \"Evaluate the company’s approach to mitigating operational risks.\",\n",
    "    \"Summarize risks related to the company’s leadership transitions.\",\n",
    "    \"Highlight financial risks related to the company’s capital structure.\",\n",
    "    \"What are the potential risks associated with the company’s debt?\",\n",
    "    \"Identify any environmental liabilities discussed in the filing.\",\n",
    "    \"Assess the company’s readiness for changes in tax law.\",\n",
    "    \"What are the key factors contributing to the company’s credit risk?\",\n",
    "    \"Analyze the company’s approach to managing legal risks.\",\n",
    "    \"Highlight risks related to compliance with labor laws.\",\n",
    "    \"What are the financial implications of disclosed risks?\",\n",
    "    \"Assess the company’s approach to managing reputation risk.\",\n",
    "    \"Summarize the company’s governance structure and related risks.\",\n",
    "    \"Identify risks related to competition in the company’s industry.\",\n",
    "    \"Analyze risks associated with the company’s expansion strategy.\",\n",
    "    \"Evaluate how the company mitigates risks from geopolitical tensions.\",\n",
    "    \"Summarize the company’s approach to managing climate-related risks.\",\n",
    "    \"Detect any emerging risks in the company’s business environment.\",\n",
    "    \"What are the risks associated with the company’s reliance on technology?\",\n",
    "    \"Highlight any risks identified in the company’s corporate social responsibility (CSR) reports.\",\n",
    "    \"Identify risks associated with intellectual property infringement.\",\n",
    "    \"Summarize risks related to compliance with the GDPR.\",\n",
    "    \"Evaluate the company’s exposure to risks from commodity price volatility.\",\n",
    "    \"What operational risks are associated with the company’s logistics network?\",\n",
    "    \"Identify key market risks affecting the company’s performance.\",\n",
    "    \"Analyze risks arising from changes in consumer behavior.\",\n",
    "    \"Summarize risks related to the company’s reliance on key suppliers.\",\n",
    "    \"Identify risks related to changes in the regulatory landscape for healthcare.\",\n",
    "    \"Evaluate risks associated with the company’s use of third-party vendors.\",\n",
    "    \"What are the risks related to the company’s employee compensation plans?\",\n",
    "    \"Summarize risks identified in the company’s sustainability reports.\",\n",
    "    \"Identify risks related to the company’s reliance on renewable energy.\",\n",
    "    \"Assess the company’s approach to managing risks from natural disasters.\",\n",
    "    \"What are the risks related to the company’s strategic investments?\",\n",
    "    \"Summarize the risks associated with the company’s real estate holdings.\",\n",
    "    \"What are the company’s plans to address emerging regulatory risks?\",\n",
    "    \"Detect risks associated with the company’s reliance on digital marketing.\",\n",
    "    \"What are the risks associated with the company’s international operations?\",\n",
    "    \"Summarize the company’s approach to managing workforce-related risks.\",\n",
    "    \"Highlight risks related to the company’s pension liabilities.\",\n",
    "    \"Identify risks related to potential supply shortages.\",\n",
    "    \"What are the risks associated with the company’s cybersecurity measures?\",\n",
    "    \"Assess the company’s compliance with industry-specific regulations.\",\n",
    "    \"Summarize risks related to the company’s research and development activities.\",\n",
    "    \"What are the potential risks related to the company’s legal disputes?\",\n",
    "    \"Identify risks related to changes in consumer privacy laws.\",\n",
    "    \"What are the emerging risks in the company’s market?\",\n",
    "    \"Summarize risks related to the company’s debt refinancing efforts.\",\n",
    "    \"Identify risks associated with fluctuations in raw material prices.\",\n",
    "    \"Assess risks related to the company’s foreign investment strategies.\",\n",
    "    \"Summarize the company’s risk management approach to emerging markets.\",\n",
    "    \"Evaluate the company’s risk management framework against best practices.\"\n",
    "]\n",
    "\n",
    "# Experimental parameters\n",
    "num_tests = 3          # Number of test runs per prompt for statistical reliability\n",
    "win_threshold = 0.5    # BERTScore threshold for considering a response \"successful\"\n",
    "\n",
    "# Ensure model is in evaluation mode to disable dropout and other training-specific behaviors\n",
    "model.eval()\n",
    "\n",
    "# Initialize results container\n",
    "results = []\n",
    "\n",
    "# Iterate through each test prompt\n",
    "for prompt in prompts:\n",
    "    # Initialize performance metric tracking for current prompt\n",
    "    time_taken_list = []        # Response generation time in seconds\n",
    "    tokens_per_second_list = [] # Throughput measurement\n",
    "    perplexity_list = []        # Model confidence/fluency metric\n",
    "    rouge_scores = []           # Content overlap metric\n",
    "    bleu_scores = []            # Content precision metric  \n",
    "    edit_distance_list = []     # String similarity metric\n",
    "    bert_score_list = []        # Semantic similarity metric\n",
    "    win_rate_list = []          # Binary success/failure metric\n",
    "\n",
    "    # Run multiple tests for each prompt to account for generation stochasticity\n",
    "    for _ in range(num_tests):\n",
    "        # Start timing the response generation\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Prepare input and move to GPU if available\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generate response using the model\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            output = model.generate(**inputs, max_new_tokens=64, temperature=0.7)\n",
    "\n",
    "        # Decode the generated token IDs back to text\n",
    "        decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Calculate response time\n",
    "        time_taken = time.time() - start_time\n",
    "        time_taken_list.append(time_taken)\n",
    "\n",
    "        # Calculate throughput (tokens per second)\n",
    "        num_tokens = len(inputs[\"input_ids\"][0]) + 64  # Input tokens + generated tokens\n",
    "        tokens_per_second = num_tokens / time_taken\n",
    "        tokens_per_second_list.append(tokens_per_second)\n",
    "\n",
    "        # Calculate perplexity (lower is better, indicates higher confidence)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss).item()\n",
    "            perplexity_list.append(perplexity)\n",
    "\n",
    "        # Calculate ROUGE score (content overlap)\n",
    "        rouge_result = rouge.compute(predictions=[decoded_output], references=[prompt])\n",
    "        rouge_scores.append(rouge_result[\"rougeL\"])\n",
    "\n",
    "        # Calculate BLEU score (precision-based similarity)\n",
    "        bleu_result = bleu.compute(predictions=[decoded_output], references=[prompt])\n",
    "        bleu_scores.append(bleu_result[\"bleu\"])\n",
    "\n",
    "        # Calculate Levenshtein edit distance (character-level similarity)\n",
    "        edit_distance = levenshtein_distance(decoded_output, prompt)\n",
    "        edit_distance_list.append(edit_distance)\n",
    "\n",
    "        # Calculate BERTScore (contextualized semantic similarity)\n",
    "        P, R, F1 = bert_score([decoded_output], [prompt], lang=\"en\", rescale_with_baseline=True)\n",
    "        f1_score = F1.mean().item()\n",
    "        bert_score_list.append(f1_score)\n",
    "        \n",
    "        # Determine if this response meets the quality threshold (binary success metric)\n",
    "        win_rate_list.append(1 if f1_score >= win_threshold else 0)\n",
    "\n",
    "    # Aggregate metrics across test runs for this prompt\n",
    "    results.append({\n",
    "        \"prompt\": prompt,\n",
    "        \"avg_time\": sum(time_taken_list) / num_tests,\n",
    "        \"avg_tps\": sum(tokens_per_second_list) / num_tests,\n",
    "        \"avg_perplexity\": sum(perplexity_list) / num_tests,\n",
    "        \"avg_rouge\": sum(rouge_scores) / num_tests,\n",
    "        \"avg_bleu\": sum(bleu_scores) / num_tests,\n",
    "        \"avg_edit_distance\": sum(edit_distance_list) / num_tests,\n",
    "        \"avg_bert_score\": sum(bert_score_list) / num_tests,\n",
    "        \"win_rate\": sum(win_rate_list) / num_tests * 100  # Convert to percentage\n",
    "    })\n",
    "\n",
    "# Create a Word document for the evaluation report\n",
    "doc = Document()\n",
    "doc.add_heading('LLM Compliance Evaluation Report', 0)\n",
    "\n",
    "# Add metadata about the experiment\n",
    "doc.add_paragraph(f\"Model: {model.__class__.__name__}\")\n",
    "doc.add_paragraph(f\"Number of test runs per prompt: {num_tests}\")\n",
    "doc.add_paragraph(f\"BERTScore Win Threshold: {win_threshold}\")\n",
    "\n",
    "# Create a structured table for results\n",
    "table = doc.add_table(rows=1, cols=9)\n",
    "table.style = 'Table Grid'\n",
    "\n",
    "# Define table headers\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'Prompt'\n",
    "hdr_cells[1].text = 'Time (s)'\n",
    "hdr_cells[2].text = 'Tokens/sec'\n",
    "hdr_cells[3].text = 'Perplexity'\n",
    "hdr_cells[4].text = 'ROUGE-L'\n",
    "hdr_cells[5].text = 'BLEU'\n",
    "hdr_cells[6].text = 'Edit Dist'\n",
    "hdr_cells[7].text = 'BERTScore'\n",
    "hdr_cells[8].text = 'Win Rate (%)'\n",
    "\n",
    "# Populate the table with evaluation results\n",
    "for r in results:\n",
    "    row_cells = table.add_row().cells\n",
    "    row_cells[0].text = r['prompt']\n",
    "    row_cells[1].text = f\"{r['avg_time']:.2f}\"\n",
    "    row_cells[2].text = f\"{r['avg_tps']:.2f}\"\n",
    "    row_cells[3].text = f\"{r['avg_perplexity']:.2f}\"\n",
    "    row_cells[4].text = f\"{r['avg_rouge']:.4f}\"\n",
    "    row_cells[5].text = f\"{r['avg_bleu']:.4f}\"\n",
    "    row_cells[6].text = f\"{r['avg_edit_distance']:.2f}\"\n",
    "    row_cells[7].text = f\"{r['avg_bert_score']:.4f}\"\n",
    "    row_cells[8].text = f\"{r['win_rate']:.2f}\"\n",
    "\n",
    "# Save the report document\n",
    "doc.save(\"compliance_mistral_finetuned_evaluation.docx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
